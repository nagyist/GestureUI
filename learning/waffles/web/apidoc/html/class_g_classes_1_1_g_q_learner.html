<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
<title>GClasses: GClasses::GQLearner Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css">
<link href="doxygen.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.5.8 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul>
      <li><a href="main.html"><span>Main&nbsp;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
  <div class="tabs">
    <ul>
      <li><a href="annotated.html"><span>Class&nbsp;List</span></a></li>
      <li><a href="classes.html"><span>Class&nbsp;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&nbsp;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&nbsp;Members</span></a></li>
    </ul>
  </div>
  <div class="navpath"><a class="el" href="namespace_g_classes.html">GClasses</a>::<a class="el" href="class_g_classes_1_1_g_q_learner.html">GQLearner</a>
  </div>
</div>
<div class="contents">
<h1>GClasses::GQLearner Class Reference</h1><!-- doxytag: class="GClasses::GQLearner" --><!-- doxytag: inherits="GClasses::GPolicyLearner" -->The base class of a Q-Learner. To use this class, there are four abstract methods you'll need to implement. See also the comment for <a class="el" href="class_g_classes_1_1_g_policy_learner.html" title="This is the base class for algorithms that learn a policy.">GPolicyLearner</a>.  
<a href="#_details">More...</a>
<p>
<code>#include &lt;GReinforcement.h&gt;</code>
<p>
<div class="dynheader">
Inheritance diagram for GClasses::GQLearner:</div>
<div class="dynsection">

<p><center><img src="class_g_classes_1_1_g_q_learner.png" usemap="#GClasses::GQLearner_map" border="0" alt=""></center>
<map name="GClasses::GQLearner_map">
<area href="class_g_classes_1_1_g_policy_learner.html" alt="GClasses::GPolicyLearner" shape="rect" coords="0,0,233,24">
<area href="class_g_classes_1_1_g_incremental_learner_q_agent.html" alt="GClasses::GIncrementalLearnerQAgent" shape="rect" coords="0,112,233,136">
</map>
</div>

<p>
<a href="class_g_classes_1_1_g_q_learner-members.html">List of all members.</a><table border="0" cellpadding="0" cellspacing="0">
<tr><td></td></tr>
<tr><td colspan="2"><br><h2>Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#4a3fbd07012a939e8f0a3e19b1aca741">GQLearner</a> (<a class="el" href="class_g_classes_1_1smart__ptr.html">sp_relation</a> &amp;pRelation, int actionDims, double *pInitialState, <a class="el" href="class_g_classes_1_1_g_rand.html">GRand</a> *pRand, <a class="el" href="class_g_classes_1_1_g_agent_action_iterator.html">GAgentActionIterator</a> *pActionIterator)</td></tr>

<tr><td class="memItemLeft" nowrap align="right" valign="top">virtual&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#f04d9ad3015d406fae0a1a339c43bd87">~GQLearner</a> ()</td></tr>

<tr><td class="memItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#818802e429a17efe90310623026f1c19">setLearningRate</a> (double d)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Sets the learning rate (often called "alpha"). If state is deterministic and actions have deterministic consequences, then this should be 1. If there is any non-determinism, there are three common approaches for picking the learning rate: 1- use a fairly small value (perhaps 0.1), 2- decay it over time (by calling this method before every iteration), 3- remember how many times 'n' each state has already been visited, and set the learning rate to 1/(n+1) before each iteration. The third technique is the best, but is awkward with continuous state spaces.  <a href="#818802e429a17efe90310623026f1c19"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#019fae7b0500332ee9947cc0e3cb1362">setDiscountFactor</a> (double d)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Sets the factor for discounting future rewards (often called "gamma").  <a href="#019fae7b0500332ee9947cc0e3cb1362"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">virtual double&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#85b97fc5300546db37e92098ddd069f9">getQValue</a> (const double *pState, const double *pAction)=0</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">You must implement some kind of structure to store q-values. This method should return the current q-value for the specified state and action.  <a href="#85b97fc5300546db37e92098ddd069f9"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">virtual void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#3d03ef30220ecb29362a77515a51fb7e">setQValue</a> (const double *pState, const double *pAction, double qValue)=0</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">This is the complement to GetQValue.  <a href="#3d03ef30220ecb29362a77515a51fb7e"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">virtual void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#2acb4fc62c4c5863e940e3d85ce1e089">refinePolicyAndChooseNextAction</a> (const double *pSenses, double *pOutActions)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">See <a class="el" href="class_g_classes_1_1_g_policy_learner.html#4aefa73ddefd3144f5a64ded2c774315" title="This method tells the agent to learn from the current senses, and select a new action...">GPolicyLearner::refinePolicyAndChooseNextAction</a>.  <a href="#2acb4fc62c4c5863e940e3d85ce1e089"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#3dc50ef1679e8ea8c7e9057ee0cde2b4">setActionCap</a> (int n)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">This specifies a cap on how many actions to sample. (If actions are continuous, you obviously don't want to try them all.).  <a href="#3dc50ef1679e8ea8c7e9057ee0cde2b4"></a><br></td></tr>
<tr><td colspan="2"><br><h2>Protected Member Functions</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">virtual void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#40f05909e96c2dd4990a6a242dcc6453">chooseAction</a> (const double *pSenses, double *pOutActions)=0</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">This method picks the action during training. This method is called by refinePolicyAndChooseNextAction. (If it makes things easier, the agent may actually perform the action here, but it's a better practise to wait until refinePolicyAndChooseNextAction returns, because that keeps the "thinking" and "acting" stages separated from each other.) One way to pick the next action is to call GetQValue for all possible actions in the current state, and pick the one with the highest Q-value. But if you always pick the best action, you'll never discover things you don't already know about, so you need to find some balance between exploration and exploitation. One way to do this is to usually pick the best action, but sometimes pick a random action.  <a href="#40f05909e96c2dd4990a6a242dcc6453"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">virtual double&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#e4ebe241f6bd5055a73e93c444ad54e0">rewardFromLastAction</a> ()=0</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">A reward is obtained when the agent performs a particular action in a particular state. (A penalty is a negative reward. A reward of zero is no reward.) This method returns the reward that was obtained when the last action was performed. If you return UNKNOWN_REAL_VALUE, then the q-table will not be updated for that action.  <a href="#e4ebe241f6bd5055a73e93c444ad54e0"></a><br></td></tr>
<tr><td colspan="2"><br><h2>Protected Attributes</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_rand.html">GRand</a> *&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#f55d40710e61201cf7cc286f973498c5">m_pRand</a></td></tr>

<tr><td class="memItemLeft" nowrap align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_agent_action_iterator.html">GAgentActionIterator</a> *&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#fd7fa567aab42860ec1c756557027f7d">m_pActionIterator</a></td></tr>

<tr><td class="memItemLeft" nowrap align="right" valign="top">double&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#4e93f86d57c064d4d451165cef670024">m_learningRate</a></td></tr>

<tr><td class="memItemLeft" nowrap align="right" valign="top">double&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#168f9083ea8f02667b920bf299d9bfe5">m_discountFactor</a></td></tr>

<tr><td class="memItemLeft" nowrap align="right" valign="top">double *&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#ec9a8a3b5cd0f70855777aa129d5545c">m_pSenses</a></td></tr>

<tr><td class="memItemLeft" nowrap align="right" valign="top">double *&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#8fa363f886412d77f698be9a3ad4e5ba">m_pAction</a></td></tr>

<tr><td class="memItemLeft" nowrap align="right" valign="top">int&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#10bac91947799c42bfde50bdf9e8efd6">m_actionCap</a></td></tr>

</table>
<hr><a name="_details"></a><h2>Detailed Description</h2>
The base class of a Q-Learner. To use this class, there are four abstract methods you'll need to implement. See also the comment for <a class="el" href="class_g_classes_1_1_g_policy_learner.html" title="This is the base class for algorithms that learn a policy.">GPolicyLearner</a>. <hr><h2>Constructor &amp; Destructor Documentation</h2>
<a class="anchor" name="4a3fbd07012a939e8f0a3e19b1aca741"></a><!-- doxytag: member="GClasses::GQLearner::GQLearner" ref="4a3fbd07012a939e8f0a3e19b1aca741" args="(sp_relation &amp;pRelation, int actionDims, double *pInitialState, GRand *pRand, GAgentActionIterator *pActionIterator)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">GClasses::GQLearner::GQLearner           </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1smart__ptr.html">sp_relation</a> &amp;&nbsp;</td>
          <td class="paramname"> <em>pRelation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&nbsp;</td>
          <td class="paramname"> <em>actionDims</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double *&nbsp;</td>
          <td class="paramname"> <em>pInitialState</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_rand.html">GRand</a> *&nbsp;</td>
          <td class="paramname"> <em>pRand</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_agent_action_iterator.html">GAgentActionIterator</a> *&nbsp;</td>
          <td class="paramname"> <em>pActionIterator</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<a class="anchor" name="f04d9ad3015d406fae0a1a339c43bd87"></a><!-- doxytag: member="GClasses::GQLearner::~GQLearner" ref="f04d9ad3015d406fae0a1a339c43bd87" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual GClasses::GQLearner::~GQLearner           </td>
          <td>(</td>
          <td class="paramname">          </td>
          <td>&nbsp;)&nbsp;</td>
          <td><code> [virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<hr><h2>Member Function Documentation</h2>
<a class="anchor" name="40f05909e96c2dd4990a6a242dcc6453"></a><!-- doxytag: member="GClasses::GQLearner::chooseAction" ref="40f05909e96c2dd4990a6a242dcc6453" args="(const double *pSenses, double *pOutActions)=0" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GQLearner::chooseAction           </td>
          <td>(</td>
          <td class="paramtype">const double *&nbsp;</td>
          <td class="paramname"> <em>pSenses</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double *&nbsp;</td>
          <td class="paramname"> <em>pOutActions</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [protected, pure virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
This method picks the action during training. This method is called by refinePolicyAndChooseNextAction. (If it makes things easier, the agent may actually perform the action here, but it's a better practise to wait until refinePolicyAndChooseNextAction returns, because that keeps the "thinking" and "acting" stages separated from each other.) One way to pick the next action is to call GetQValue for all possible actions in the current state, and pick the one with the highest Q-value. But if you always pick the best action, you'll never discover things you don't already know about, so you need to find some balance between exploration and exploitation. One way to do this is to usually pick the best action, but sometimes pick a random action. 
<p>

<p>Implemented in <a class="el" href="class_g_classes_1_1_g_incremental_learner_q_agent.html#21ae8e8bae94fbc24d797f61f5cbc6c5">GClasses::GIncrementalLearnerQAgent</a>.</p>

</div>
</div><p>
<a class="anchor" name="85b97fc5300546db37e92098ddd069f9"></a><!-- doxytag: member="GClasses::GQLearner::getQValue" ref="85b97fc5300546db37e92098ddd069f9" args="(const double *pState, const double *pAction)=0" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual double GClasses::GQLearner::getQValue           </td>
          <td>(</td>
          <td class="paramtype">const double *&nbsp;</td>
          <td class="paramname"> <em>pState</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double *&nbsp;</td>
          <td class="paramname"> <em>pAction</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [pure virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
You must implement some kind of structure to store q-values. This method should return the current q-value for the specified state and action. 
<p>

<p>Implemented in <a class="el" href="class_g_classes_1_1_g_incremental_learner_q_agent.html#408da3ccfb79be4da4ce5b94e135417a">GClasses::GIncrementalLearnerQAgent</a>.</p>

</div>
</div><p>
<a class="anchor" name="2acb4fc62c4c5863e940e3d85ce1e089"></a><!-- doxytag: member="GClasses::GQLearner::refinePolicyAndChooseNextAction" ref="2acb4fc62c4c5863e940e3d85ce1e089" args="(const double *pSenses, double *pOutActions)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GQLearner::refinePolicyAndChooseNextAction           </td>
          <td>(</td>
          <td class="paramtype">const double *&nbsp;</td>
          <td class="paramname"> <em>pSenses</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double *&nbsp;</td>
          <td class="paramname"> <em>pOutActions</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
See <a class="el" href="class_g_classes_1_1_g_policy_learner.html#4aefa73ddefd3144f5a64ded2c774315" title="This method tells the agent to learn from the current senses, and select a new action...">GPolicyLearner::refinePolicyAndChooseNextAction</a>. 
<p>

<p>Implements <a class="el" href="class_g_classes_1_1_g_policy_learner.html#4aefa73ddefd3144f5a64ded2c774315">GClasses::GPolicyLearner</a>.</p>

</div>
</div><p>
<a class="anchor" name="e4ebe241f6bd5055a73e93c444ad54e0"></a><!-- doxytag: member="GClasses::GQLearner::rewardFromLastAction" ref="e4ebe241f6bd5055a73e93c444ad54e0" args="()=0" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual double GClasses::GQLearner::rewardFromLastAction           </td>
          <td>(</td>
          <td class="paramname">          </td>
          <td>&nbsp;)&nbsp;</td>
          <td><code> [protected, pure virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
A reward is obtained when the agent performs a particular action in a particular state. (A penalty is a negative reward. A reward of zero is no reward.) This method returns the reward that was obtained when the last action was performed. If you return UNKNOWN_REAL_VALUE, then the q-table will not be updated for that action. 
<p>

</div>
</div><p>
<a class="anchor" name="3dc50ef1679e8ea8c7e9057ee0cde2b4"></a><!-- doxytag: member="GClasses::GQLearner::setActionCap" ref="3dc50ef1679e8ea8c7e9057ee0cde2b4" args="(int n)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GQLearner::setActionCap           </td>
          <td>(</td>
          <td class="paramtype">int&nbsp;</td>
          <td class="paramname"> <em>n</em>          </td>
          <td>&nbsp;)&nbsp;</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
This specifies a cap on how many actions to sample. (If actions are continuous, you obviously don't want to try them all.). 
<p>

</div>
</div><p>
<a class="anchor" name="019fae7b0500332ee9947cc0e3cb1362"></a><!-- doxytag: member="GClasses::GQLearner::setDiscountFactor" ref="019fae7b0500332ee9947cc0e3cb1362" args="(double d)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GQLearner::setDiscountFactor           </td>
          <td>(</td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>d</em>          </td>
          <td>&nbsp;)&nbsp;</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Sets the factor for discounting future rewards (often called "gamma"). 
<p>

</div>
</div><p>
<a class="anchor" name="818802e429a17efe90310623026f1c19"></a><!-- doxytag: member="GClasses::GQLearner::setLearningRate" ref="818802e429a17efe90310623026f1c19" args="(double d)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GQLearner::setLearningRate           </td>
          <td>(</td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>d</em>          </td>
          <td>&nbsp;)&nbsp;</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Sets the learning rate (often called "alpha"). If state is deterministic and actions have deterministic consequences, then this should be 1. If there is any non-determinism, there are three common approaches for picking the learning rate: 1- use a fairly small value (perhaps 0.1), 2- decay it over time (by calling this method before every iteration), 3- remember how many times 'n' each state has already been visited, and set the learning rate to 1/(n+1) before each iteration. The third technique is the best, but is awkward with continuous state spaces. 
<p>

</div>
</div><p>
<a class="anchor" name="3d03ef30220ecb29362a77515a51fb7e"></a><!-- doxytag: member="GClasses::GQLearner::setQValue" ref="3d03ef30220ecb29362a77515a51fb7e" args="(const double *pState, const double *pAction, double qValue)=0" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GQLearner::setQValue           </td>
          <td>(</td>
          <td class="paramtype">const double *&nbsp;</td>
          <td class="paramname"> <em>pState</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double *&nbsp;</td>
          <td class="paramname"> <em>pAction</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>qValue</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [pure virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
This is the complement to GetQValue. 
<p>

<p>Implemented in <a class="el" href="class_g_classes_1_1_g_incremental_learner_q_agent.html#e02583c57303a86e45af9f631bbed665">GClasses::GIncrementalLearnerQAgent</a>.</p>

</div>
</div><p>
<hr><h2>Member Data Documentation</h2>
<a class="anchor" name="10bac91947799c42bfde50bdf9e8efd6"></a><!-- doxytag: member="GClasses::GQLearner::m_actionCap" ref="10bac91947799c42bfde50bdf9e8efd6" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int <a class="el" href="class_g_classes_1_1_g_q_learner.html#10bac91947799c42bfde50bdf9e8efd6">GClasses::GQLearner::m_actionCap</a><code> [protected]</code>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<a class="anchor" name="168f9083ea8f02667b920bf299d9bfe5"></a><!-- doxytag: member="GClasses::GQLearner::m_discountFactor" ref="168f9083ea8f02667b920bf299d9bfe5" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double <a class="el" href="class_g_classes_1_1_g_q_learner.html#168f9083ea8f02667b920bf299d9bfe5">GClasses::GQLearner::m_discountFactor</a><code> [protected]</code>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<a class="anchor" name="4e93f86d57c064d4d451165cef670024"></a><!-- doxytag: member="GClasses::GQLearner::m_learningRate" ref="4e93f86d57c064d4d451165cef670024" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double <a class="el" href="class_g_classes_1_1_g_q_learner.html#4e93f86d57c064d4d451165cef670024">GClasses::GQLearner::m_learningRate</a><code> [protected]</code>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<a class="anchor" name="8fa363f886412d77f698be9a3ad4e5ba"></a><!-- doxytag: member="GClasses::GQLearner::m_pAction" ref="8fa363f886412d77f698be9a3ad4e5ba" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double* <a class="el" href="class_g_classes_1_1_g_q_learner.html#8fa363f886412d77f698be9a3ad4e5ba">GClasses::GQLearner::m_pAction</a><code> [protected]</code>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<a class="anchor" name="fd7fa567aab42860ec1c756557027f7d"></a><!-- doxytag: member="GClasses::GQLearner::m_pActionIterator" ref="fd7fa567aab42860ec1c756557027f7d" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_agent_action_iterator.html">GAgentActionIterator</a>* <a class="el" href="class_g_classes_1_1_g_q_learner.html#fd7fa567aab42860ec1c756557027f7d">GClasses::GQLearner::m_pActionIterator</a><code> [protected]</code>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<a class="anchor" name="f55d40710e61201cf7cc286f973498c5"></a><!-- doxytag: member="GClasses::GQLearner::m_pRand" ref="f55d40710e61201cf7cc286f973498c5" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_rand.html">GRand</a>* <a class="el" href="class_g_classes_1_1_g_q_learner.html#f55d40710e61201cf7cc286f973498c5">GClasses::GQLearner::m_pRand</a><code> [protected]</code>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<a class="anchor" name="ec9a8a3b5cd0f70855777aa129d5545c"></a><!-- doxytag: member="GClasses::GQLearner::m_pSenses" ref="ec9a8a3b5cd0f70855777aa129d5545c" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double* <a class="el" href="class_g_classes_1_1_g_q_learner.html#ec9a8a3b5cd0f70855777aa129d5545c">GClasses::GQLearner::m_pSenses</a><code> [protected]</code>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
</div>
<hr size="1"><address style="text-align: right;"><small>Generated on Tue Nov 2 14:18:24 2010 for GClasses by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.5.8 </small></address>
</body>
</html>
