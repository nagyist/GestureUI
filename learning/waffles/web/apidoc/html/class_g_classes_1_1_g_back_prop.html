<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
<title>GClasses: GClasses::GBackProp Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css">
<link href="doxygen.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.5.8 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul>
      <li><a href="main.html"><span>Main&nbsp;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
  <div class="tabs">
    <ul>
      <li><a href="annotated.html"><span>Class&nbsp;List</span></a></li>
      <li><a href="classes.html"><span>Class&nbsp;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&nbsp;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&nbsp;Members</span></a></li>
    </ul>
  </div>
  <div class="navpath"><a class="el" href="namespace_g_classes.html">GClasses</a>::<a class="el" href="class_g_classes_1_1_g_back_prop.html">GBackProp</a>
  </div>
</div>
<div class="contents">
<h1>GClasses::GBackProp Class Reference</h1><!-- doxytag: class="GClasses::GBackProp" -->This class performs backpropagation on a neural network. (It is a separate class, because it is only needed while training. There is no reason to waste this space after training is complete, or if you choose to use a different technique to train the neural network.).  
<a href="#_details">More...</a>
<p>
<code>#include &lt;GNeuralNet.h&gt;</code>
<p>

<p>
<a href="class_g_classes_1_1_g_back_prop-members.html">List of all members.</a><table border="0" cellpadding="0" cellspacing="0">
<tr><td></td></tr>
<tr><td colspan="2"><br><h2>Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#defd54b49a91ee4abf6215a20cb74f5d">GBackProp</a> (<a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> *pNN)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">This class will adjust the weights in pNN.  <a href="#defd54b49a91ee4abf6215a20cb74f5d"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#2fdca5057035e0cd9ee2ee238d2d47aa">~GBackProp</a> ()</td></tr>

<tr><td class="memItemLeft" nowrap align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> &amp;&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#369b2823257508f2b98a00ff62125801">layer</a> (int layer)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Returns a layer (not a layer of the neural network, but a corresponding layer of values used for back-prop).  <a href="#369b2823257508f2b98a00ff62125801"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#71d0f533c7ae13baf0780a5ac8aa9e57">backpropagate</a> ()</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">This method assumes that the error term is already set at every unit in the output layer. It uses back-propagation to compute the error term at every hidden unit. (It does not update any weights.).  <a href="#71d0f533c7ae13baf0780a5ac8aa9e57"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#b298b55432dbab879cf663700a48d1b4">descendGradient</a> (const double *pFeatures, double learningRate, double momentum)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">This method assumes that the error term is alrady set for every network unit. It adjusts weights to descend the gradient of the error surface with respect to the weights.  <a href="#b298b55432dbab879cf663700a48d1b4"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#b0deed4abe6e745c35c2232fd40678db">adjustFeatures</a> (double *pFeatures, double learningRate, size_t skip=0)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">This method assumes that the error term is alrady set for every network unit. It descends the gradient by adjusting the features (not the weights).  <a href="#b0deed4abe6e745c35c2232fd40678db"></a><br></td></tr>
<tr><td colspan="2"><br><h2>Static Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">static void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#4648bfa8f89eb1d6abd757f7d36491bb">backPropLayer</a> (<a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNFromLayer, <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNToLayer, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPFromLayer, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPToLayer, size_t fromBegin=0)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Backpropagates the error from the "from" layer to the "to" layer. (If the "to" layer has fewer units than the "from" layer, then it will begin propagating with the (fromBegin+1)th weight and stop when the "to" layer runs out of units. It would be an error if the number of units in the "from" layer is less than the number of units in the "to" layer plus fromBegin.  <a href="#4648bfa8f89eb1d6abd757f7d36491bb"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">static void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#da65debf6ae8626751969a9e330a0773">backPropLayer2</a> (<a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNFromLayer1, <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNFromLayer2, <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNToLayer, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPFromLayer1, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPFromLayer2, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPToLayer, int pass)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">This is another implementation of backPropLayer. This one is somewhat more flexible, but slightly less efficient. It supports backpropagating error from one or two layers. (pNNFromLayer2 should be NULL if you are backpropagating from just one layer.) It also supports temporal backpropagation by unfolding in time and then averaging the error across all of the unfolded instantiations. "pass" specifies how much of the error for this pass to accept. 1=all of it, 2=half of it, 3=one third, etc.  <a href="#da65debf6ae8626751969a9e330a0773"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">static void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#91971cd69d87e8cce0a2d18bc37c3b46">adjustWeights</a> (<a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNFromLayer, <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNToLayer, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPFromLayer, double learningRate, double momentum)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Adjust weights in pNNFromLayer. (The error for pNNFromLayer layer must have already been computed.) (If you are backpropagating error from two layers, you can just call this method twice, once for each previous layer.).  <a href="#91971cd69d87e8cce0a2d18bc37c3b46"></a><br></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">static void&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a060b61ff8f99fae8e6b849dbf181a57">adjustWeights</a> (<a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNFromLayer, const double *pFeatures, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPFromLayer, double learningRate, double momentum)</td></tr>

<tr><td class="mdescLeft">&nbsp;</td><td class="mdescRight">Adjust weights in pNNFromLayer. (The error for pNNFromLayer layer must have already been computed.) (If you are backpropagating error from two layers, you can just call this method twice, once for each previous layer.).  <a href="#a060b61ff8f99fae8e6b849dbf181a57"></a><br></td></tr>
<tr><td colspan="2"><br><h2>Protected Attributes</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> *&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#f1484d812296d23a0b332942f5bfff73">m_pNN</a></td></tr>

<tr><td class="memItemLeft" nowrap align="right" valign="top">std::vector&lt; <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> &gt;&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#5e01ba2da853068b910c71072459ec0e">m_layers</a></td></tr>

<tr><td colspan="2"><br><h2>Friends</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">class&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#9311a6752671c2a8e3cf8d8aacff043e">GNeuralNet</a></td></tr>

</table>
<hr><a name="_details"></a><h2>Detailed Description</h2>
This class performs backpropagation on a neural network. (It is a separate class, because it is only needed while training. There is no reason to waste this space after training is complete, or if you choose to use a different technique to train the neural network.). <hr><h2>Constructor &amp; Destructor Documentation</h2>
<a class="anchor" name="defd54b49a91ee4abf6215a20cb74f5d"></a><!-- doxytag: member="GClasses::GBackProp::GBackProp" ref="defd54b49a91ee4abf6215a20cb74f5d" args="(GNeuralNet *pNN)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">GClasses::GBackProp::GBackProp           </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> *&nbsp;</td>
          <td class="paramname"> <em>pNN</em>          </td>
          <td>&nbsp;)&nbsp;</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
This class will adjust the weights in pNN. 
<p>

</div>
</div><p>
<a class="anchor" name="2fdca5057035e0cd9ee2ee238d2d47aa"></a><!-- doxytag: member="GClasses::GBackProp::~GBackProp" ref="2fdca5057035e0cd9ee2ee238d2d47aa" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">GClasses::GBackProp::~GBackProp           </td>
          <td>(</td>
          <td class="paramname">          </td>
          <td>&nbsp;)&nbsp;</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<hr><h2>Member Function Documentation</h2>
<a class="anchor" name="b0deed4abe6e745c35c2232fd40678db"></a><!-- doxytag: member="GClasses::GBackProp::adjustFeatures" ref="b0deed4abe6e745c35c2232fd40678db" args="(double *pFeatures, double learningRate, size_t skip=0)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::adjustFeatures           </td>
          <td>(</td>
          <td class="paramtype">double *&nbsp;</td>
          <td class="paramname"> <em>pFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&nbsp;</td>
          <td class="paramname"> <em>skip</em> = <code>0</code></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
This method assumes that the error term is alrady set for every network unit. It descends the gradient by adjusting the features (not the weights). 
<p>

</div>
</div><p>
<a class="anchor" name="a060b61ff8f99fae8e6b849dbf181a57"></a><!-- doxytag: member="GClasses::GBackProp::adjustWeights" ref="a060b61ff8f99fae8e6b849dbf181a57" args="(GNeuralNetLayer *pNNFromLayer, const double *pFeatures, GBackPropLayer *pBPFromLayer, double learningRate, double momentum)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">static void GClasses::GBackProp::adjustWeights           </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pNNFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double *&nbsp;</td>
          <td class="paramname"> <em>pFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pBPFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>momentum</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [static]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Adjust weights in pNNFromLayer. (The error for pNNFromLayer layer must have already been computed.) (If you are backpropagating error from two layers, you can just call this method twice, once for each previous layer.). 
<p>

</div>
</div><p>
<a class="anchor" name="91971cd69d87e8cce0a2d18bc37c3b46"></a><!-- doxytag: member="GClasses::GBackProp::adjustWeights" ref="91971cd69d87e8cce0a2d18bc37c3b46" args="(GNeuralNetLayer *pNNFromLayer, GNeuralNetLayer *pNNToLayer, GBackPropLayer *pBPFromLayer, double learningRate, double momentum)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">static void GClasses::GBackProp::adjustWeights           </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pNNFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pNNToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pBPFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>momentum</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [static]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Adjust weights in pNNFromLayer. (The error for pNNFromLayer layer must have already been computed.) (If you are backpropagating error from two layers, you can just call this method twice, once for each previous layer.). 
<p>

</div>
</div><p>
<a class="anchor" name="71d0f533c7ae13baf0780a5ac8aa9e57"></a><!-- doxytag: member="GClasses::GBackProp::backpropagate" ref="71d0f533c7ae13baf0780a5ac8aa9e57" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::backpropagate           </td>
          <td>(</td>
          <td class="paramname">          </td>
          <td>&nbsp;)&nbsp;</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
This method assumes that the error term is already set at every unit in the output layer. It uses back-propagation to compute the error term at every hidden unit. (It does not update any weights.). 
<p>

</div>
</div><p>
<a class="anchor" name="4648bfa8f89eb1d6abd757f7d36491bb"></a><!-- doxytag: member="GClasses::GBackProp::backPropLayer" ref="4648bfa8f89eb1d6abd757f7d36491bb" args="(GNeuralNetLayer *pNNFromLayer, GNeuralNetLayer *pNNToLayer, GBackPropLayer *pBPFromLayer, GBackPropLayer *pBPToLayer, size_t fromBegin=0)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">static void GClasses::GBackProp::backPropLayer           </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pNNFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pNNToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pBPFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pBPToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&nbsp;</td>
          <td class="paramname"> <em>fromBegin</em> = <code>0</code></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [static]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Backpropagates the error from the "from" layer to the "to" layer. (If the "to" layer has fewer units than the "from" layer, then it will begin propagating with the (fromBegin+1)th weight and stop when the "to" layer runs out of units. It would be an error if the number of units in the "from" layer is less than the number of units in the "to" layer plus fromBegin. 
<p>

</div>
</div><p>
<a class="anchor" name="da65debf6ae8626751969a9e330a0773"></a><!-- doxytag: member="GClasses::GBackProp::backPropLayer2" ref="da65debf6ae8626751969a9e330a0773" args="(GNeuralNetLayer *pNNFromLayer1, GNeuralNetLayer *pNNFromLayer2, GNeuralNetLayer *pNNToLayer, GBackPropLayer *pBPFromLayer1, GBackPropLayer *pBPFromLayer2, GBackPropLayer *pBPToLayer, int pass)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">static void GClasses::GBackProp::backPropLayer2           </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pNNFromLayer1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pNNFromLayer2</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pNNToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pBPFromLayer1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pBPFromLayer2</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&nbsp;</td>
          <td class="paramname"> <em>pBPToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&nbsp;</td>
          <td class="paramname"> <em>pass</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [static]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
This is another implementation of backPropLayer. This one is somewhat more flexible, but slightly less efficient. It supports backpropagating error from one or two layers. (pNNFromLayer2 should be NULL if you are backpropagating from just one layer.) It also supports temporal backpropagation by unfolding in time and then averaging the error across all of the unfolded instantiations. "pass" specifies how much of the error for this pass to accept. 1=all of it, 2=half of it, 3=one third, etc. 
<p>

</div>
</div><p>
<a class="anchor" name="b298b55432dbab879cf663700a48d1b4"></a><!-- doxytag: member="GClasses::GBackProp::descendGradient" ref="b298b55432dbab879cf663700a48d1b4" args="(const double *pFeatures, double learningRate, double momentum)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::descendGradient           </td>
          <td>(</td>
          <td class="paramtype">const double *&nbsp;</td>
          <td class="paramname"> <em>pFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>momentum</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
This method assumes that the error term is alrady set for every network unit. It adjusts weights to descend the gradient of the error surface with respect to the weights. 
<p>

</div>
</div><p>
<a class="anchor" name="369b2823257508f2b98a00ff62125801"></a><!-- doxytag: member="GClasses::GBackProp::layer" ref="369b2823257508f2b98a00ff62125801" args="(int layer)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a>&amp; GClasses::GBackProp::layer           </td>
          <td>(</td>
          <td class="paramtype">int&nbsp;</td>
          <td class="paramname"> <em>layer</em>          </td>
          <td>&nbsp;)&nbsp;</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Returns a layer (not a layer of the neural network, but a corresponding layer of values used for back-prop). 
<p>

</div>
</div><p>
<hr><h2>Friends And Related Function Documentation</h2>
<a class="anchor" name="9311a6752671c2a8e3cf8d8aacff043e"></a><!-- doxytag: member="GClasses::GBackProp::GNeuralNet" ref="9311a6752671c2a8e3cf8d8aacff043e" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">friend class <a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a><code> [friend]</code>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<hr><h2>Member Data Documentation</h2>
<a class="anchor" name="5e01ba2da853068b910c71072459ec0e"></a><!-- doxytag: member="GClasses::GBackProp::m_layers" ref="5e01ba2da853068b910c71072459ec0e" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;<a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a>&gt; <a class="el" href="class_g_classes_1_1_g_back_prop.html#5e01ba2da853068b910c71072459ec0e">GClasses::GBackProp::m_layers</a><code> [protected]</code>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
<a class="anchor" name="f1484d812296d23a0b332942f5bfff73"></a><!-- doxytag: member="GClasses::GBackProp::m_pNN" ref="f1484d812296d23a0b332942f5bfff73" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a>* <a class="el" href="class_g_classes_1_1_g_back_prop.html#f1484d812296d23a0b332942f5bfff73">GClasses::GBackProp::m_pNN</a><code> [protected]</code>          </td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>

</div>
</div><p>
</div>
<hr size="1"><address style="text-align: right;"><small>Generated on Tue Nov 2 14:18:24 2010 for GClasses by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.5.8 </small></address>
</body>
</html>
